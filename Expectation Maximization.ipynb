{"cells":[{"metadata":{"_uuid":"fc2647a248e94d589830b9d7c300e19d5ec034c9"},"cell_type":"markdown","source":"Welcome to my notebook on Kaggle. I did record my notes so it might help others in their journey to understand Machine Learning / Neural Networks by examples. This notebook is my way to contribute back to the Kaggle platform and community.\n\nI noticed that a lot of terminology and algorithems are thrown around and often some good math to underpin why it is correct. I can read, study and even understand it but after lot's of other terminology and weeks passing I catch myself often going back to understand it again because it didn't \"stick\". Doing exercises, work with the algorithm and trying out for myself helps me to overcome this, learning by example. So for Expectation Mazimization I created this notebook and try it for myself in a simple example. \n\nThe reason why I needed to understand expectation Maximization is because it was mentioned in research work on biological inspired true artificial intelligence where neuroscience produced a candidate which suggests that several global brain theories might be unified within a free-energy framework: Free Energy Principle (FEP) by Karl Friston: The free-energy principle is an attempt to explain the structure and function of the brain, starting from the very fact that we exist. See my notebook:  \nhttps://www.kaggle.com/charel/learn-by-example-active-inference-in-the-brain-1\n\n\n"},{"metadata":{"_uuid":"b599a51f9715ac53083c707c2f365ee4f0acc0da"},"cell_type":"markdown","source":"# Expectation Maximization\nIn statistics, an expectation–maximization (EM) algorithm is an iterative method to find maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.\n\nSource: [Wikipedia](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm)\n"},{"metadata":{"_uuid":"fa89cb5dec11daf8aa8e0bca9cc965769f6f03ea"},"cell_type":"markdown","source":"# How the EM algorithem works\n\nPlease have a look at this short video where it is explained very well on a 1-dimensional example: https://www.youtube.com/watch?v=iQoXFmbXRJA  \n  \n    \n    \n\n![](http://www.wilsonmongwe.co.za/wp-content/uploads/2015/07/400px-EM.jpg)\n\nBelow slides explaining with a 2-dimensional example\n![alt text](http://i.imgur.com/0cTmOja.png \"Logo Title Text 1\")\n\n![alt text](http://i.imgur.com/8kQ9aa6.png \"Logo Title Text 1\")\n\n![alt text](http://i.imgur.com/A1j3lqB.png \"Logo Title Text 1\")"},{"metadata":{"_uuid":"1527cec278f530e10d5cc4356bfcd1121422bd34"},"cell_type":"markdown","source":"## Basic example of Expectation Maximization \n\nLet's try this on a simple basic example.  \nObjective: Let's etimate the probability distribution in a 1-dimensional dataset \n*  that we generated ourselves using  2 gaussian normal probability distributions.\n*  Using a Gaussian mixture model with 2 normal gaussian distributions\n\nOr in other words, we should be able to find our own mean and standard deviation parameters we used to generate the testset.\n\nNote, this notebook is for building the basic conceptual understanding of the EM algorithm. Not to have the best code, the best computable algorithm, etc. I tried to keep it simple.\n"},{"metadata":{"_uuid":"2c4a77cfebf99a55462a377ad2930fa6f05eefde"},"cell_type":"markdown","source":"## Gaussian Mixture Model of 2 Gaussians\nTwo Normal distributions $N(\\mu_1, \\sigma_1^2)$ and $N(\\mu_2, \\sigma_2^2)$. \n\nThere are 5 paramaters to estimate: $\\theta = (w, \\mu_1, \\sigma_1^2, \\mu_2, \\sigma_2^2)$ where $w$ is the probability that the data comes from the first normal probability distribution  (and 1-$w$) it comes from the second normal probability distribution) \n\nThe probability density function (PDF) of the mixture model is:\n\n$$f(x | \\theta) = w \\  f_1(x \\ | \\  \\mu_1, \\sigma_1^2) + (1 - w) \\  f_2 (x  \\ | \\  \\mu_2, \\sigma_2^2) $$  \n\nObjective is to best fit a given probability density by finding $\\theta = (w, \\mu_1, \\sigma_1^2, \\mu_2, \\sigma_2^2)$ through EM iterations\n\nLet try it out (code credits go to [ brianspiering](https://github.com/brianspiering/gaussian_mixture_models/blob/master/intro_to_gmm_%26_em.ipynb))\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# import libraries\n\n# For plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"white\")\n%matplotlib inline\n#for matrix math\nimport numpy as np\n#for normalization + probability density function computation\nfrom scipy import stats\n#for data preprocessing\nimport pandas as pd\nfrom math import sqrt, log, exp, pi\nfrom random import uniform\nprint(\"import done\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"598f2539ced3f159ba773bb33b0e3456c00739ce"},"cell_type":"markdown","source":"![](http://)# Generate the data yourself \nSelect $\\mu_1, \\sigma_1$ and $\\mu_2, \\sigma_2$ to generate the data\n"},{"metadata":{"trusted":true,"_uuid":"fc48f5c29294186a1a6a09408e6d7e108653aa4d"},"cell_type":"code","source":"random_seed=36788765\nnp.random.seed(random_seed)\n\nMean1 = 2.0  # Input parameter, mean of first normal probability distribution\nStandard_dev1 = 4.0 #@param {type:\"number\"}\nMean2 = 9.0 # Input parameter, mean of second normal  probability distribution\nStandard_dev2 = 2.0 #@param {type:\"number\"}\n\n# generate data\ny1 = np.random.normal(Mean1, Standard_dev1, 1000)\ny2 = np.random.normal(Mean2, Standard_dev2, 500)\ndata=np.append(y1,y2)\n\n# For data visiualisation calculate left and right of the graph\nMin_graph = min(data)\nMax_graph = max(data)\nx = np.linspace(Min_graph, Max_graph, 2000) # to plot the data\n\nprint('Input Gaussian {:}: μ = {:.2}, σ = {:.2}'.format(\"1\", Mean1, Standard_dev1))\nprint('Input Gaussian {:}: μ = {:.2}, σ = {:.2}'.format(\"2\", Mean2, Standard_dev2))\nsns.distplot(data, bins=20, kde=False);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8396cbb53bd12807ec38d131bf17bc78d0ab5fbc"},"cell_type":"code","source":"class Gaussian:\n    \"Model univariate Gaussian\"\n    def __init__(self, mu, sigma):\n        #mean and standard deviation\n        self.mu = mu\n        self.sigma = sigma\n\n    #probability density function\n    def pdf(self, datum):\n        \"Probability of a data point given the current parameters\"\n        u = (datum - self.mu) / abs(self.sigma)\n        y = (1 / (sqrt(2 * pi) * abs(self.sigma))) * exp(-u * u / 2)\n        return y\n    \n    def __repr__(self):\n        return 'Gaussian({0:4.6}, {1:4.6})'.format(self.mu, self.sigma)\nprint(\"done\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09fe1994901d7ac9a130698c4148cb00b5c4b1da"},"cell_type":"markdown","source":"# A single Gaussion will not fit the data well\nCalculating the mean and standard deviation of the dataset shows it does not fit well"},{"metadata":{"trusted":true,"_uuid":"d206d152f8682ae4117a0d436c04747c1095897c"},"cell_type":"code","source":"#gaussian of best fit\nbest_single = Gaussian(np.mean(data), np.std(data))\nprint('Best single Gaussian: μ = {:.2}, σ = {:.2}'.format(best_single.mu, best_single.sigma))\n#fit a single gaussian curve to the data\ng_single = stats.norm(best_single.mu, best_single.sigma).pdf(x)\nsns.distplot(data, bins=20, kde=False, norm_hist=True);\nplt.plot(x, g_single, label='single gaussian');\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be42e26212b2227bde625a210bc11710972bb303"},"cell_type":"markdown","source":"# The code for EM with 2 Gaussian mixture model"},{"metadata":{"trusted":true,"_uuid":"605def3ac170f3f8d1892aa54866b17412a1709c"},"cell_type":"code","source":"class GaussianMixture_self:\n    \"Model mixture of two univariate Gaussians and their EM estimation\"\n\n    def __init__(self, data, mu_min=min(data), mu_max=max(data), sigma_min=1, sigma_max=1, mix=.5):\n        self.data = data\n        #todo the Algorithm would be numerical enhanced by normalizing the data first, next do all the EM steps and do the de-normalising at the end\n        \n        #init with multiple gaussians\n        self.one = Gaussian(uniform(mu_min, mu_max), \n                            uniform(sigma_min, sigma_max))\n        self.two = Gaussian(uniform(mu_min, mu_max), \n                            uniform(sigma_min, sigma_max))\n        \n        #as well as how much to mix them\n        self.mix = mix\n\n    def Estep(self):\n        \"Perform an E(stimation)-step, assign each point to gaussian 1 or 2 with a percentage\"\n        # compute weights\n        self.loglike = 0. # = log(p = 1)\n        for datum in self.data:  \n            # unnormalized weights\n            wp1 = self.one.pdf(datum) * self.mix\n            wp2 = self.two.pdf(datum) * (1. - self.mix)\n            # compute denominator\n            den = wp1 + wp2\n            # normalize\n            wp1 /= den   \n            wp2 /= den     # wp1+wp2= 1, it either belongs to gaussian 1 or gaussion 2\n            # add into loglike\n            self.loglike += log(den) #freshening up self.loglike in the process\n            # yield weight tuple\n            yield (wp1, wp2)\n\n    def Mstep(self, weights):\n        \"Perform an M(aximization)-step\"\n        # compute denominators\n        (left, rigt) = zip(*weights) \n        one_den = sum(left)\n        two_den = sum(rigt)\n\n        # compute new means\n        self.one.mu = sum(w * d  for (w, d) in zip(left, data)) / one_den\n        self.two.mu = sum(w * d  for (w, d) in zip(rigt, data)) / two_den\n        \n        # compute new sigmas\n        self.one.sigma = sqrt(sum(w * ((d - self.one.mu) ** 2)\n                                  for (w, d) in zip(left, data)) / one_den)\n        self.two.sigma = sqrt(sum(w * ((d - self.two.mu) ** 2)\n                                  for (w, d) in zip(rigt, data)) / two_den)\n        # compute new mix\n        self.mix = one_den / len(data)\n\n        \n    def iterate(self, N=1, verbose=False):\n        \"Perform N iterations, then compute log-likelihood\"\n        for i in range(1, N+1):\n            self.Mstep(self.Estep()) #The heart of the algorith, perform E-stepand next M-step\n            if verbose:\n                print('{0:2} {1}'.format(i, self))\n        self.Estep() # to freshen up self.loglike\n\n    def pdf(self, x):\n        return (self.mix)*self.one.pdf(x) + (1-self.mix)*self.two.pdf(x)\n        \n    def __repr__(self):\n        return 'GaussianMixture({0}, {1}, mix={2.03})'.format(self.one, \n                                                              self.two, \n                                                              self.mix)\n\n    def __str__(self):\n        return 'Mixture: {0}, {1}, mix={2:.03})'.format(self.one, \n                                                        self.two, \n                                                        self.mix)\nprint(\"done\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37cb2937ff2a86a94f9687ec28373c8c13c813d6"},"cell_type":"code","source":"# See the algorithem in action\nn_iterations = 20\nbest_mix = None\nbest_loglike = float('-inf')\nmix = GaussianMixture_self(data)\nfor _ in range(n_iterations):\n    try:\n        #train!\n        mix.iterate(verbose=True)\n        if mix.loglike > best_loglike:\n            best_loglike = mix.loglike\n            best_mix = mix\n        \n    except (ZeroDivisionError, ValueError, RuntimeWarning): # Catch division errors from bad starts, and just throw them out...\n        print(\"one less\")\n        pass\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f54ecb1bd2e5bb370c80b1c1d6f5bdcebf3eb076"},"cell_type":"code","source":"# Find best Mixture Gaussian model\nn_iterations = 300\nn_random_restarts = 4\nbest_mix = None\nbest_loglike = float('-inf')\nprint('Computing best model with random restarts...\\n')\nfor _ in range(n_random_restarts):\n    mix = GaussianMixture_self(data)\n    for _ in range(n_iterations):\n        try:\n            mix.iterate()\n            if mix.loglike > best_loglike:\n                best_loglike = mix.loglike\n                best_mix = mix\n        except (ZeroDivisionError, ValueError, RuntimeWarning): # Catch division errors from bad starts, and just throw them out...\n            pass\n#print('Best Gaussian Mixture : μ = {:.2}, σ = {:.2} with μ = {:.2}, σ = {:.2}'.format(best_mix.one.mu, best_mix.one.sigma, best_mix.two.mu, best_mix.two.sigma))\n\nprint('Input Gaussian {:}: μ = {:.2}, σ = {:.2}'.format(\"1\", Mean1, Standard_dev1))\nprint('Input Gaussian {:}: μ = {:.2}, σ = {:.2}'.format(\"2\", Mean2, Standard_dev2))\nprint('Gaussian {:}: μ = {:.2}, σ = {:.2}, weight = {:.2}'.format(\"1\", best_mix.one.mu, best_mix.one.sigma, best_mix.mix))\nprint('Gaussian {:}: μ = {:.2}, σ = {:.2}, weight = {:.2}'.format(\"2\", best_mix.two.mu, best_mix.two.sigma, (1-best_mix.mix)))\n#Show mixture\nsns.distplot(data, bins=20, kde=False, norm_hist=True);\ng_both = [best_mix.pdf(e) for e in x]\nplt.plot(x, g_both, label='gaussian mixture');\ng_left = [best_mix.one.pdf(e) * best_mix.mix for e in x]\nplt.plot(x, g_left, label='gaussian one');\ng_right = [best_mix.two.pdf(e) * (1-best_mix.mix) for e in x]\nplt.plot(x, g_right, label='gaussian two');\nplt.legend();\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fab10e6ee3acf1b9d35e766f9300225b9cfb2a7c"},"cell_type":"markdown","source":"# Results\nThe models nicely estimates our own mean entered μ and σ. With this understanding it is not hard to imagine to extend this to n-Gaussians or m-dimensions. You don't need to write complax code since these algorithms have been implemented in some excellent libraries.\n\n### Further enhancements to the code\nThe Algorithm would be numerical enhanced by normalizing the data first, next do all the EM steps and do the de-normalising at the end. For me (and guess others) I needed to get to base-camp first and get the EM steps understood. \n\n## sklearn GaussianMixture \nOr we could make use of a library that already has the functionality implemented. The sklearn GaussianMixture object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models.  A GaussianMixture.fit method is provided that learns a Gaussian Mixture Model from train data. Now you can try yourself with n-Gaussians or m-dimensions."},{"metadata":{"trusted":true,"_uuid":"8d2203f981d40741f39d9552f45baba450171b65"},"cell_type":"code","source":"from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components = 2, tol=0.000001)\ngmm.fit(np.expand_dims(data, 1)) # Parameters: array-like, shape (n_samples, n_features), 1 dimension dataset so 1 feature\nGaussian_nr = 1\nprint('Input Gaussian {:}: μ = {:.2}, σ = {:.2}'.format(\"1\", Mean1, Standard_dev1))\nprint('Input Gaussian {:}: μ = {:.2}, σ = {:.2}'.format(\"2\", Mean2, Standard_dev2))\nfor mu, sd, p in zip(gmm.means_.flatten(), np.sqrt(gmm.covariances_.flatten()), gmm.weights_):\n    print('Gaussian {:}: μ = {:.2}, σ = {:.2}, weight = {:.2}'.format(Gaussian_nr, mu, sd, p))\n    g_s = stats.norm(mu, sd).pdf(x) * p\n    plt.plot(x, g_s, label='gaussian sklearn');\n    Gaussian_nr += 1\nsns.distplot(data, bins=20, kde=False, norm_hist=True)\ngmm_sum = np.exp([gmm.score_samples(e.reshape(-1, 1)) for e in x]) #gmm gives log probability, hence the exp() function\nplt.plot(x, gmm_sum, label='gaussian mixture');\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01733447abb25ad754e0cbc9f7df2c2b466dd98c"},"cell_type":"markdown","source":"## If you want to explore this subject further\n\nIf you want to have a more formal understand of Gaussian mixture, I would recommend:  \nhttps://www.youtube.com/watch?v=4vGiHC35j9s   \nif you want to have a less formal, please watch:  \nhttps://www.youtube.com/watch?v=JNlEIEwe-Cg  \nEM Demystified: An Expectation-Maximization Tutorial:  \nhttps://vannevar.ece.uw.edu/techsite/papers/documents/UWEETR-2010-0002.pdf  \n\n"},{"metadata":{},"cell_type":"markdown","source":"Hope you liked my notebook (upvote top right), my way to conribute back to this fantastic Kaggle platform and community.\nSome of my other notebooks:  \nhttps://www.kaggle.com/charel/learn-by-example-how-many-neurons-layers  \nhttps://www.kaggle.com/charel/learn-by-example-reinforcement-learning-with-gym  \nhttps://www.kaggle.com/charel/learn-by-example-active-inference-in-the-brain-1  "}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}